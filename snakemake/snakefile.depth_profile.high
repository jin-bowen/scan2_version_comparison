# vim: syntax=python

# The memory limit here can be insufficient if the size of each
# GATK chunk is large (>10MB or so).
rule depth_profile_region_summarize_per_sample:
    input:
        tab="depth_profile/joint_depth_matrix.tab.gz",
        tabidx="depth_profile/joint_depth_matrix.tab.gz.tbi"
    output:
        rda="depth_profile/{sample}_depth_table.rda"
    params:
        sc_sample="{sample}",
        bulk_sample=config['bulk_sample'],
        genome=config['genome']
    benchmark:
        "depth_profile/benchmark_{sample}_region_summarize.txt"
    resources:
        # This memory value depends strongly on the size of GATK regions. Here we
        # assume 10 Mb chunks and 10 bytes per position per sample.
        #
        # it is unclear to me why this much RAM is needed when reading in chunks
        # and reading only 2 samples at a time, but these values are derived from
        # many real world runs. the max. memory used was 1555 MB per thread; did
        # not scale with number of samples in each batch, as expected. give 1800 MB,
        # a little extra headroom.
        mem=lambda wildcards, input, threads: 1800 * threads
    threads: config['digest_depth_n_cores']
    shell:
        """
        {config[scripts]}/digest_depth.R {input.tab} \
            {params.sc_sample} {params.bulk_sample} {params.genome} \
            {output.rda} {threads}
        """


rule callable_region_gather:
    input:
        expand("depth_profile/gatk_depthofcoverage/chunk{gatk_chunk}.txt",
            gatk_chunk=range(1, config['gatk_chunks']+1))
    output:
        tabgz="depth_profile/joint_depth_matrix.tab.gz",
        tabidx="depth_profile/joint_depth_matrix.tab.gz.tbi"
    benchmark:
        "depth_profile/benchmark_joint_depth_matrix.txt"
    threads: 4
    resources:
        mem=1000
    shell:
        """
        (grep '^#' {input[0]} ; cat {input} | grep -v '^#') \
            | bgzip --threads {threads} -c > {output.tabgz}
        tabix -p vcf -S 1 {output.tabgz}
        """


# Needs to match parallelization of GATK scattering, else the
# callable regions will not match the actual genotyped area.
# Also needs to match GATK filtering (mapping quality, additional
# default HaplotypeCaller read filters) to be meaningful.
rule callable_region_scatter:
    input:
        scbam=config['bam_map'].values()
    output:
        tmp=temp("depth_profile/gatk_depthofcoverage/chunk{gatk_chunk}.tmp.txt"),
        txt="depth_profile/gatk_depthofcoverage/chunk{gatk_chunk}.txt"
    params:
        bams=" ".join([ "-I " + bam for bam in config['bam_map'].values() ]),
        regionflag=lambda wildcards:
            "-L " + config['gatk_regions'][int(wildcards.gatk_chunk) - 1],
    benchmark:
        "depth_profile/gatk_depthofcoverage/benchmark_depthofcoverage_chunk{gatk_chunk}.txt"
    threads: 1
    resources:
        mem=6000
    shell:
        """
        gatk DepthOfCoverage \
            --java-options '-Xmx5G -Xms5G' \
            -R {config[ref]} \
            {params.bams} \
            {params.regionflag} \
            --minimum-mapping-quality 60 \
            --read-filter NotSecondaryAlignmentReadFilter \
            --read-filter GoodCigarReadFilter \
            --read-filter NonZeroReferenceLengthAlignmentReadFilter \
            --read-filter PassesVendorQualityCheckReadFilter \
            --read-filter MappedReadFilter \
            --read-filter MappingQualityAvailableReadFilter \
            --read-filter NotDuplicateReadFilter \
            --read-filter MappingQualityReadFilter \
            --read-filter WellformedReadFilter \
            --omit-interval-statistics true \
            --omit-locus-table true \
            --omit-per-sample-statistics true \
            --output-format TABLE \
            -O {output.tmp}
        {config[scripts]}/totab.depth_profile.sh {output.tmp} {output.txt} \
        """
