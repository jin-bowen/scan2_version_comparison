#!/usr/bin/env python
# vim: set syntax=python

import math
import glob
import subprocess
import argparse
import os.path
import re
import yaml
import os
import pwd
import time
import uuid
import pysam       # just for reading BAM headers to find samples
import snakemake
import distutils.util # for strtobool
from sys import exit


# The default set must include ALL POSSIBLE parameters, even ones like BAM
# lists that will be empty at first.  This is because the default dict keys
# are used to determine which command line arguments are not to be stored
# in the configuration database.
scan2_param_defaults = {
    'analysis': 'call_mutations',
    'genome': 'hs37d5',
    'is_started': False,
    'gatk': 'gatk3_joint',
    'parsimony_phasing': False,
    'target_fdr': 0.01,
    'rescue_target_fdr': 0.01,
    'snv_min_sc_alt': 2,
    'snv_min_sc_dp': 6,
    'snv_min_bulk_dp': 11, 
    'snv_max_bulk_alt': 0, 
    'snv_max_bulk_mosaic_vaf': 0.3,
    'indel_min_sc_alt': 2,
    'indel_min_sc_dp': 10,
    'indel_min_bulk_dp': 11, 
    'indel_max_bulk_alt': 0, 
    'indel_max_bulk_mosaic_vaf': 0.3,
    'min_base_quality_score': 20,
    'analyze_snvs': True,
    'analyze_indels': True,
    'analyze_mosaic_snvs': False,
    'callable_regions': True,
    'abmodel_use_fit': {},
    'abmodel_chunk_strategy': False,
    'abmodel_chunks': 10,
    'abmodel_samples_per_step': 20000,
    'abmodel_hsnp_tile_size': 100,
    'abmodel_hsnp_n_tiles': 250,
    'abmodel_refine_steps': 4,
    'abmodel_n_cores': 20,
    'digest_depth_n_cores': 20,
    'genotype_n_cores': 20,
    'integrate_table_n_cores': 20,
    'makepanel_n_cores': 20,
    'rescue_n_cores': 12,
    'resample_M': 20,
    'mimic_legacy': False,
    'bulk_sample': None,
    'gatk_chunks': None,
    'gatk_regions': [],
    'chrs': [],
    'phaser': 'shapeit',
    'shapeit_refpanel': None,  # a directory, so readable_file fails
    'cross_sample_panel': None, # must be readable IF specified, but pipeline should run without it too
    'scripts': '/n/data1/bch/genetics/lee/shulin/softwares/anaconda3/envs/scan2/lib/scan2',  # also a directory
    'chr_prefix': '',          # be nice to remove this; requires snakefile changes
    'bam_map': {},             # maps sample name -> bam path
    'permtool_config_map': {},
    'permtool_matrix_map': {},
    'permtool_n_permutations': 10000,
    'permtool_callable_bed_n_cores': 20,
    'permtool_make_permutations_n_cores': 20,
    'permtool_combine_permutations_n_cores': 20
}

# Unlike params, all files listed here must be readable upon a call
# to validate() or run().
scan2_file_defaults = {
    'snakefile': '/n/data1/bch/genetics/lee/shulin/softwares/anaconda3/envs/scan2/lib/scan2/Snakefile',
    'ref': None,
    'dbsnp': None,
    'bams': {},         # miscellaneous BAMs; not genotyped as single cells
    'sc_bams': {},      # BAMs for single cells, will be genotyped
    'bulk_bam': None,   # single bulk BAM for phasing, filtering germlines
    'scan2_objects': {},
    'add_muts': None,
    'makepanel_metadata': None,
    'permtool_muts': None,
    'permtool_bedtools_genome_file': None,
    'gatk_vcf': None,
    'eagle_refpanel': {},
    'eagle_genmap': None
}

scan2_defaults = { **scan2_param_defaults, **scan2_file_defaults }


class Analysis:
    """
    A SCAN2 analysis: metadata and actions.  Instantiation requires
    loading configuration data (and potentially Snakemake runtime
    data) from disk.
    """

    def __init__(self, analysis_dir):
        self.analysis_dir = analysis_dir
        self.cluster_log_dir = Analysis.make_logs_path(self.analysis_dir)
        self.config_path = Analysis.make_config_path(self.analysis_dir)
        self.load()

    @staticmethod
    def make_config_path(path):
        return os.path.abspath(os.path.join(path, 'scan.yaml'))

    @staticmethod
    def make_logs_path(path):
        return os.path.abspath(os.path.join(path, 'cluster-logs'))


    @staticmethod
    def create(args):
        """
        Create a new analysis object on disk and return an Analysis
        object pointing to it. Caution: will overwrite a previously
        existing SCAN2 analysis.
        Analysis is initialized with default parameters.
        """
        cpath = Analysis.make_config_path(args.analysis_dir)

        os.makedirs(args.analysis_dir, exist_ok=True)
        os.makedirs(Analysis.make_logs_path(args.analysis_dir),
            exist_ok=True)
        analysis_uuid = uuid.uuid4()   # A random ID
        ct = time.time()
        cd = time.strftime('%Y-%m-%d %H:%M %Z', time.localtime(ct))
        cfg = { 'creator': pwd.getpwuid(os.getuid()).pw_name,
                'create_time': ct,
                'create_date': cd,
                'analysis_uuid': str(analysis_uuid),
                **scan2_defaults
        }

        with open(cpath, 'w') as yfile:
            yaml.dump(cfg, yfile, default_flow_style=False)

        # Return the newly created object
        return Analysis(args.analysis_dir)


    def is_started(self):
        return self.cfg['is_started']


    def __str__(self):
        return "%s SCAN2 analysis type=%s (change with `config --analysis`), ID=%s" % \
            ('LOCKED' if self.is_started() else 'UNLOCKED',
             str(self.analysis),
             str(self.analysis_uuid))


    def show(self, verbose=False):
        """If 'verbose' is given, show all configuration values."""
        if verbose:
            return '\n'.join([ str(self) ] + \
                [ "%25s: %s" % (str(k), str(v)) \
                for k, v in self.cfg.items() ])
        else:
            return '\n'.join([ str(self),
                '%15s: %d single cell(s), %d bulk(s), %d other' % \
                    ('BAMs', len(self.cfg['sc_bams']),
                      self.cfg['bulk_bam'] is not None,
                      len(self.cfg['bams'])),
                '%15s: %s' % ('Creator', self.cfg['creator']),
                '%15s: %s' % ('Create date', self.cfg['create_date']) ])


    def load(self):
        """Requires self.config_path to already be set."""
        with open(self.config_path, 'r') as yf:
            self.cfg = yaml.load(yf, Loader=yaml.FullLoader)
        self.analysis = self.cfg['analysis']
        self.analysis_uuid = self.cfg['analysis_uuid']


    def configure(self, args, verbose=False):
        """
        Merge new parameter values from 'args' into the previous config
        dictionary. New parameter specifications override old ones.  Since
        these values are not changeable once an analysis begins running,
        there is no issue with overwriting old values.

        'args' is the result of ArgumentParser.  Argument values will be
        None unless they were specified on the command line in *this*
        invocation of the scan2 script.
        """
        # First pass: get all new arguments and perform any special
        # handling. Second pass will tally updates and inform.
        new_cfg = {}
        for k, v in vars(args).items():
            if v is not None:
                for k2, v2 in self.handle_special(k, v).items():
                    new_cfg[k2] = v2
        
        # unusually special updates
        # 1. make the default value for chrs/gatk
        # regions when the user specifies the reference FASTA. But only
        # do this is the user did not also specify their own gatk regions
        # via --regions or --region-file in this call AND gatk_regions was
        # not set in a previous configure().
        if 'ref' in new_cfg and \
            'gatk_regions' not in new_cfg and self.cfg['gatk_regions'] == []:
            autosomes = get_autosomes_from_ref(new_cfg['ref'])
            new_cfg = { **new_cfg, **Analysis.handle_regions(autosomes) } # dict merge

        updates = 0
        for k, v in new_cfg.items():
            if k in self.cfg.keys() and v is not None and self.cfg[k] != v:
                if args.verbose:
                    print("Updating %s: %s -> %s" % (k, str(self.cfg[k]), str(v)))
                if k in scan2_file_defaults and type(v) == str:
                    # only top-level files are automatically converted,
                    # deeper files require handle_special().
                    v = readable_file(v)   # returns the absolute path
                self.cfg[k] = v
                updates = updates + 1

        # 2. Special update: sample -> bam map
        if updates > 0:
            tups = []
            if self.cfg['bulk_bam'] is not None:
                tups.append((self.cfg['bulk_sample'], self.cfg['bulk_bam']))
            if len(self.cfg['sc_bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['sc_bams'].items() ]
            if len(self.cfg['bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['bams'].items() ]
            if len(tups) > 0:
                self.cfg['bam_map'] = dict(tups)
                print(self.cfg['bam_map'])

        if updates == 0:
            print("No changes to make to configuration. Stopping.")
        else:
            with open(self.config_path, 'w') as yf:
                yaml.dump(self.cfg, yf, default_flow_style=False)


    def handle_special(self, key, value):
        """
        Allow special handling of configuration parameters.  Any command
        line argument may expand into an arbitrarily large dict to be added
        to the configuration database.
        To skip special handling, simply return the original key and value.
        """
        if key in [ 'bam', 'sc_bam', 'bulk_bam', 'scan2_object' ]:
            if key in [ 'bam', 'sc_bam' ]:  # these provide lists
                return { key + 's' :
                    dict([ (sample_from_bam(bam), readable_file(bam)) for bam in value ]) }
            if key == 'bulk_bam':
                return { 'bulk_bam': readable_file(value),
                         'bulk_sample': sample_from_bam(value) }
            if key == 'scan2_object':       # list with nargs=2
                return { key + 's' :
                    dict([ (sample_id, readable_file(rda)) for sample_id, rda in value ]) }

        if key == 'permtool_sample':
            return {
                'permtool_config_map' :
                    dict([ (sample_id, readable_file(self.make_config_path(scan2_dir))) for sample_id, scan2_dir in value ]),
                'permtool_matrix_map' :
                    dict([ (sample_id, readable_file(os.path.join(scan2_dir, 'depth_profile', 'joint_depth_matrix.tab.gz'))) for sample_id, scan2_dir in value ]) }

        if key in [ 'regions', 'regions_file' ]:
            regions = regions_from_string(value) if \
                key == 'regions' else regions_from_file(value)
            return Analysis.handle_regions(regions)

        if key == 'abmodel_use_fit':
            # this is a list of tuples
            return { key : dict([ (sample, readable_file(file)) for sample, file in value ]) }

        if key == 'phaser':
            return { 'phaser': value.lower() }   # Just ensure lowercase

        if key in [ 'eagle_refpanel' ]:
            # value = the path to the Eagle ref panel
            # Other sanity checks related to the reference panel are handled
            # in check_phaser().
            return { 'eagle_refpanel' : eagle_refpanel_files(value) }

        if key == 'gatk_vcf':
            # check that there is an associated VCF index file with the expected name
            readable_file(value + '.idx')
            return { 'gatk_vcf': readable_file(value) }

        # default: use the key=value pair as specified on command line
        return { key: value }


    @staticmethod
    def handle_regions(regions):
        """Extra handling for GATK genotyping intervals (regions)."""
        return { 'gatk_chunks': len(regions),
                 'gatk_regions': regions,
                 'chrs': chrs_from_regions(regions) }


    def check_scan2_objects(self):
        if len(self.cfg['scan2_objects']) == 0:
            error('no SCAN2 objects were specified (--scan2-object)')
        [ readable_file(r) for r in self.cfg['scan2_objects'].values() ]


    def check_bams(self):
        if self.cfg['bulk_bam'] is None:
            error('no bulk BAM was specified (--bulk-bam)')

        if len(self.cfg['sc_bams']) == 0:
            error('no single cell BAMs were specified (--sc-bam)')

        [ readable_file(b) for b in self.cfg['sc_bams'].values() ]
        if len(self.cfg['bams']) > 0:  # miscellaneous BAMs aren't required
            [ readable_file(b) for b in self.cfg['bams'].values() ]
        readable_file(self.cfg['bulk_bam'])


    def check_phaser(self):
        # Determine the phaser and check relevant files.
        if self.cfg['phaser'] == 'shapeit':
            check_shapeit(self.cfg['shapeit_refpanel'])
        elif self.cfg['phaser'] == 'eagle':
            check_eagle(self.cfg['eagle_refpanel'], self.cfg['eagle_genmap'])
        else:
            error("--phaser must be either 'shapeit' or 'eagle'\n")


    def validate(self):
        """
        Check that all necessary files exist and ensure the list of
        BAMs and single cell/bulk sample IDs make sense.
        """
        if self.cfg['analysis'] == 'call_mutations' or self.cfg['analysis'] == 'makepanel':
            # Additional checks: index files
            print("Checking reference genome..")
            check_refgenome(self.cfg['ref'])
            print("Checking dbSNP VCF..")
            check_dbsnp(self.cfg['dbsnp'])
    
            # Additional checks: indexes, at least one bulk and one single cell
            print("Checking BAMs..")
            self.check_bams()

            if self.cfg['analysis'] == 'call_mutations':
                # Additional checks: very specific reference panel layout
                print("Checking phasing reference panel..")
                self.check_phaser()

            if self.cfg['analysis'] == 'makepanel':
                print("Checking panel metadata..")
                readable_file(self.cfg['makepanel_metadata'])

        if self.cfg['analysis'] == 'rescue':
            print("Checking SCAN2 objects..")
            self.check_scan2_objects()

            if self.cfg['add_muts'] is not None:
                readable_file(self.cfg['add_muts'])

            fdr = float(self.cfg['rescue_target_fdr'])
            if fdr < 0 or fdr > 1:
                error('--rescue-target-fdr must be in [0, 1]')

        if self.cfg['analysis'] == 'permtool':
            print("Checking mutation file..")
            readable_file(self.cfg['permtool_muts'])

            print("Checking bedtools -g file..")
            readable_file(self.cfg['permtool_bedtools_genome_file'])

            print("Checking per-sample SCAN2 config files..")
            for sample, config in self.cfg['permtool_config_map'].items():
                readable_file(config)

            print("Checking depth matrix files..")
            for sample, matrix in self.cfg['permtool_matrix_map'].items():
                readable_file(matrix)



def error(str):
    print("ERROR: " + str)
    exit(1)


# Only checks that a file path can be read.
def readable_file(path):
    try:
        if path is None:
            raise IOError('file is not specified')
        with open(path, 'r') as f:
            return os.path.abspath(path)
    except IOError as err:
        error("file {0} could not be read:\n    {1}".format(path, err))


def check_refgenome(refgenome):
    """Check for FASTA index (.fai) and dictionary (.dict) files.
       Assumes refgenome has already been checked."""
    if refgenome is None:
        error("please provide a reference genome (--ref)")
    readable_file(refgenome + '.fai')
    readable_file(re.sub('.fasta$', '.dict', refgenome))


def get_autosomes_from_ref(refpath):
    """
    Get the correct names for chromosomes 1-22 from the FASTA header.
    ASSUMES the first 22 contigs in the header are the autosomes.
    """
    with open(refpath + '.fai', 'r') as f:
        return [ line.split('\t')[0] for line in f ][0:22]


def check_dbsnp(dbsnp):
    """Check for the VCF index file (.idx).  Assumes dbsnp has already
       been checked."""
    if dbsnp is None:
        error("please provide a dbSNP VCF (--dbsnp)")
    readable_file(dbsnp)
    readable_file(dbsnp + '.idx')


def check_shapeit(shapeit_panel):
    """Check all required SHAPEIT haplotype panel files. Exit if any are missing."""
    if shapeit_panel is None:
        error("no SHAPEIT reference panel provided (--shapeit-refpanel)")
    if not os.path.exists(shapeit_panel):
        error("SHAPEIT panel path does not exist: " + shapeit_panel)
    if not os.path.isdir(shapeit_panel):
        error("SHAPEIT panel path is not a directory; " + shapeit_panel)
    for i in range(1, 23):
        fname = 'genetic_map_chr{0}_combined_b37.txt'.format(i)
        readable_file(os.path.join(shapeit_panel, fname))
        for suf in [ 'hap', 'legend' ]:
            fname = "1000GP_Phase3_chr{0}.{1}.gz".format(i, suf)
            readable_file(os.path.join(shapeit_panel, fname))

    # chrX is not consistently named
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.hap.gz"))
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.legend.gz"))
    readable_file(os.path.join(shapeit_panel, "genetic_map_chrX_nonPAR_combined_b37.txt"))


def eagle_refpanel_files(eagle_refpanel):
    chrs = [ 'chr' + str(chrom) for chrom in list(range(1,23)) + [ 'X' ] ]
    files = {}
    for s in chrs:
        files[s] = os.path.join(eagle_refpanel, '{0}_GRCh38.genotypes.bcf'.format(s))
    return files


def check_eagle(eagle_refpanel, eagle_genmap):
    """This function is called in scan2 validate, which is AFTER the config
    database has been parsed. So eagle_refpanel is a dict() chr -> file path,
    not the path to the top level of the Eagle refpanel dir."""
    if eagle_refpanel is None:
        error("no Eagle reference panel provided (--eagle-refpanel)")

    if eagle_genmap is None:
        error("no Eagle genmap provided (--eagle-genmap)")
    if not os.path.exists(eagle_genmap):
        error("Provided Eagle genmap file does not exist: " + eagle_genmap)

    for f in eagle_refpanel.values():
        readable_file(f)
        readable_file(f + '.csi')


def sample_from_bam(bampath):
    with pysam.AlignmentFile(bampath, 'r') as af:
        sample = af.header.to_dict()['RG'][0]['SM']
    print('Got sample name "%s" from BAM %s' % (sample, bampath))
    return sample




def regions_from_string(regions):
    return args.regions.split(',')


def regions_from_file(regions_file):
    regions = []
    with open(regions_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            chrom, start, stop = line.strip().split('\t')[0:3]
            regions.append("{0}:{1}-{2}".format( chrom, int(start)+1, stop))
    return regions


def chrs_from_regions(regions):
    chrs = []
    # retains order
    rchrs = [ r.split(":")[0] for r in regions ]
    for c in rchrs:
        if chrs.count(c) == 0:
            chrs.append(c)

    return chrs






def do_init(args):
    try:
        # If an analysis exists, exception won't be thrown.
        a = Analysis(args.analysis_dir)
        print("ERROR: '%s' already contains a SCAN2 analysis." % \
            args.analysis_dir)
        print("Please delete the directory to create a new analysis.")
        print(a.show())
        exit(1)
    except FileNotFoundError:
        # Nothing on disk. Proceed.
        print("Creating new analysis with default parameters in '%s'.." % \
            args.analysis_dir)
        a = Analysis.create(args)
        print(a.show())
        print('Done.')
        print('Provide input files and set parameters via scan2 config.')
        print('Start the analysis with scan2 run.')


    
def do_config(args):
    a = Analysis(args.analysis_dir)

    if a.is_started():
        error("This analysis has already started and can no longer "
              "be configured.  If you wish to change runtime parameters, "
              "please see the 'run' subcommand.")

    a.configure(args)



def do_validate(args):
    a = Analysis(args.analysis_dir)
    a.validate()



def do_run(args):
    a = Analysis(args.analysis_dir)

    # I would like to use the snakemake API directly (i.e., call snakemake()),
    # but this makes it difficult to allow the user to supply arbitrary
    # snakemake options.
    # Must be a vector of strings as would be expected in argv[1:].
    # N.B., argv[0] is the invoking executable and should not be specified.
    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ]

    run_snakemake(snakemake_command, args, a)


# Runs a snakemake command provided by the user. Assumes 'args' contains
# runtime args.
def run_snakemake(snakemake_command, args, a):
    if args.memlimit:
        snakemake_command += [ "--resources",  "mem=" + str(args.memlimit) ]

    # handle --cluster or --drmaa and perform %logdir substitution
    if args.cluster is not None or args.drmaa is not None:
        if args.cluster is not None and args.drmaa is not None:
            error('only one of --cluster and --drmaa can be specified')
        if args.cluster is not None:
            snakemake_command += [ '--cluster',
                re.sub('%logdir', a.cluster_log_dir, args.cluster) ]
        else:
            snakemake_command += [ '--drmaa',
                re.sub('%logdir', a.cluster_log_dir, args.drmaa) ]

    # additional arbitrary snakemake args
    if args.snakemake_args:
        if args.snakemake_args[0] == ' ':
            args.snakemake_args = args.snakemake_args[1:]
        snakemake_command += args.snakemake_args.split(' ')

    snakemake.main(snakemake_command)


def do_makepanel(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ]

    run_snakemake(snakemake_command, args, a)


def do_rescue(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ]

    run_snakemake(snakemake_command, args, a)


def do_permtool(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ]

    run_snakemake(snakemake_command, args, a)


def do_show(args):
    a = Analysis(args.analysis_dir)
    print(a.show(args.verbose))


# Would prefer not to have to parse this output, but learning to use the
# snakemake API directly is a project for a later time.
def parse_snakemake_job_report(process):
    jobdict = {}
    indata = False
    for line in process.stdout.decode().split('\n'):
        line = line.strip()
        if line == '':
            continue

        if line == 'Job counts:' or line == 'count\tjobs':
            indata = True
            continue

        if indata:
            try:
                count, rulename = line.split('\t')
                jobdict[rulename] = int(count)
            except ValueError:
                # Thrown on the last line, which does not have a rule name
                jobdict['__total'] = int(line)

    # when there's no jobs to run, snakemake prints nothing
    if len(jobdict) == 0:
        jobdict['__total'] = 0

    return jobdict


# 20 block progress bar
def progress_bar(x, total):
    blocks = math.floor(x/total*100/5)
    return '[' + '#'*blocks + ' '*(20-blocks) + '] %5d/%5d %0.1f%%' % (x, total, 100*x/total)


def do_progress(args):
    a = Analysis(args.analysis_dir)
    snakemake_command_status = [ "snakemake",
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--rerun-incomplete",
        "--quiet", "--dryrun" ]

    # Command above returns jobs that currently need to run.
    # Adding --forceall shows how many jobs there were to begin wtih.
    pstatus = subprocess.run(snakemake_command_status, capture_output=True)
    ptotal = subprocess.run(snakemake_command_status + [ '--forceall' ],
        capture_output=True)
    dstatus = parse_snakemake_job_report(pstatus)
    dtotal = parse_snakemake_job_report(ptotal)
    if args.verbose:
        print(a.show())
        print('')
        for k, v in dtotal.items():
            # __total is the total count, all is Snakemake's top-leel dummy rule
            if k != 'all':
                n_complete = v - dstatus.get(k, 0)
                k = 'Total progress' if k == '__total' else k
                print('%-40s %s' % (k, progress_bar(n_complete, v)))
    else:
        print('Total progress: %s' % \
            progress_bar(dtotal['__total'] - dstatus['__total'], dtotal['__total']))



##########################################################################
# Command line arguments and subcommand definitions.
##########################################################################

def add_runtime_args(parser):
    parser.add_argument('--joblimit', default=1, metavar='INT',
        help='Allow at most INT jobs to execute at any given time.  For '
            'multicore machines or clusters, this can greatly decrease '
            'runtime.')
    parser.add_argument('--memlimit', default=None, metavar='INT', type=int,
            help="Total available memory in MB.  If unspecified, memory is "
            "treated as unlimited.")
    parser.add_argument('--cluster', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --cluster parameter.  Do not use "
            "--snakemake-args to access --cluster.  Memory requirements "
            "for each job can be accessed via {resources.mem} and any "
            "instance of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--drmaa', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --drmaa parameter.  Do not use "
            "--snakemake-args to access --drmaa.  Memory requirements for "
            "each job can be accessed via {resources.mem} and any instance "
            "of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--snakemake-args', default='', type=str, metavar='STRING',
        help='Allows supplying arbitrary snakemake arguments in STRING. See snakemake --help for a list of parameters. Note that a leading space may be necessary, e.g., --snakemake-args " --dryrun".')

    return parser

if __name__ == "__main__":
    ap = argparse.ArgumentParser(prog='scan2',
        description='Somatic SNV genotyper for whole genome amplified '
                    'single cell sequencing experiments.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument('-d', '--analysis-dir', metavar='PATH', default='.',
        type=str,
        help='PATH containing the SCAN2 analysis. Required for all subcommands.')
    ap.add_argument('--snakefile', metavar='PATH', type=str,
        default='/n/data1/bch/genetics/lee/shulin/softwares/anaconda3/envs/scan2/lib/scan2/Snakefile',
        help='Path to the main Snakefile.  Unlikely to be necessary for standard use.')
    subparsers = ap.add_subparsers(dest='subcommand')


    ##########################################################################
    # The 'init' subcommand
    #
    # 'init' only creates a new repository, nothing else. It is a separate
    # command to avoid accidental overwrites.
    ##########################################################################
    init_parser = subparsers.add_parser('init',
        help='Initialize a SCAN2 analysis directory')
    init_parser.set_defaults(executor=do_init)
    
    
    ##########################################################################
    # The 'config' subcommand
    #
    # Configuration variables need to be split into ones that can be
    # modified after analysis has run vs. ones that must be set first.
    #
    # Important: overwrite protection.
    ##########################################################################
    config_parser = subparsers.add_parser('config',
        help='Change configuration of a SCAN2 analysis. Note: config parameters '
            'cannot be changed after the analysis has begun.')
    config_parser.set_defaults(executor=do_config)
    
    config_parser.add_argument('--analysis', default='call_mutations',
        choices=[ 'call_mutations', 'makepanel', 'rescue', 'permtool' ],
        help='Analysis to run.\ncall_mutations: call somatic mutations.\nmakepanel: create a cross-sample panel suitable for --cross-sample-filter.\nrescue: perform signature-based rescue on SCAN2 objects generated by call_mutations.\npermtool: create control background distributions for somatic mutations by permutation. Mutations must be preprocesed to, e.g., remove duplicates.')
    config_parser.add_argument('--verbose', action='store_true', default=False,
        help='Print detailed list of all configuration changes.')
    config_parser.add_argument('--scripts', metavar='PATH',
        default='/n/data1/bch/genetics/lee/shulin/softwares/anaconda3/envs/scan2/lib/scan2',
        help='Path to SCAN2 script files.  Usually points to an installed '
            'set of files.')
    config_parser.add_argument('--gatk', default='gatk3_joint',
        choices=[ 'gatk3_joint', 'gatk4_joint', 'gatk4_gvcf' ],
        help='GATK version and strategy for generating candidate somatic '
             'mutations and cross-sample support. Valid values are "gatk3_joint", '
             '"gatk4_joint" and "gatk4_gvcf". Joint calling means that all BAMs '
             'are provided directly to HaplotypeCaller, whereas each BAM (sample) is '
             'initially analyzed separately in the GVCF strategy and combined '
             'later. The GVCF method can be lossy since all BAM data is not stored '
             'in the initial analysis; however, it does not require rerunning GATK '
             'across all samples in the multi-sample modes. The GVCF strategy is '
             'currently experimental and we do not recommend using it.'
             'When --gatk-vcf VCF is used, the user must ensure that the chosen --gatk '
             'strategy matches the strategy used to generate VCF.')

    config_parser.add_argument('--gatk-vcf', type=str, metavar='VCF',
        help='SCAN2 runs GATK twice internally: once with a high read mapping quality '
             'threshold (mmq60) and once with a very low threshold (mmq1). Specifying '
             '--gatk-vcf skips the mmq60 run and uses VCF in its place, potentially '
             'saving a large amount of compute time. This option is especially useful '
             'when `scan2 makepanel` has been run to create a --cross-sample-panel: the '
             'GATK VCF stored in path/to/makepanel_output/gatk/hc_raw.mmq60.vcf is an '
             'appropriate input for --gatk-vcf.\n'
             'VERY IMPORTANT: VCF _must_ be generated by GATK in '
             '_exactly_ the way SCAN2 expects; this option is not meant to allow users '
             'to supply arbitrary inputs to SCAN2. DO THIS AT YOUR OWN RISK! There is '
             'no guarantee that SCAN2 will behave or even call mutations with high '
             'accuracy if the user ignores this advice and supplies a VCF generated by '
             'other means via --gatk-vcf. Furthermore, the --gatk strategy must '
             'match the strategy originally used to generate VCF.')
    config_parser.add_argument('--mimic-legacy', action='store_true', default=False,
        help="Mimic some behaviors of older SCAN2 versions. Not comprehensive.")
    config_parser.add_argument('--genotype-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to be used in the final genotyping step.")
    config_parser.add_argument('--integrate-table-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to be used when creating the integrated table.")
    config_parser.add_argument('--digest-depth-n-cores', default=20, type=int, metavar='N',
        help="Number of cores used to tabulate single cell and bulk sequencing "
             "depth.")
    config_parser.add_argument('--permtool-callable-bed-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when creating the BED of callable regions.")
    config_parser.add_argument('--permtool-make-permutations-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when generating shuffled mutation candidates.")
    config_parser.add_argument('--permtool-combine-permutations-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when gathering per-sample permutations.")
    

    # required arguments
    req = config_parser.add_argument_group("External data")
    req.add_argument('--ref', type=readable_file, metavar='PATH',
        help='Path to reference genome FASTA file.  As required by GATK, '
             'two additional files must be present: an index (.fai) and a '
             'dictionary (.dict).')
    req.add_argument('--genome', type=str,
        choices=[ 'hs37d5', 'hg38', 'mm10' ],
        help='Reference genome to use.  This is not checked against '
             'the provided reference genome or phasing panels, the user '
             'must ensure these match the supplied genome.  It is used '
             'when determining mutation signatures (e.g., to fetch '
             'trinucleotide contexts around candidate mutations).  Valid '
             'values must match those recognized by the R SCAN2 package '
             '(see the genome.string.to.bsgenome.object function).')
    req.add_argument('--dbsnp', type=readable_file, metavar='PATH',
        help='Path to a tribble-indexed (NOT tabix indexed) dbSNP VCF.')
    
    infiles = config_parser.add_argument_group('Input sequencing data',
        'At least two BAM files must be specified: one via --bulk-bam'
        ' and at least via one via --sc-bam.  Additional BAMs can be '
        ' specified by --bam: these will be used for GATK joint calling,'
        'but will affect the analysis in no other way.  This can be '
        'useful for adding additional bulks for followup comparisons.'
        'IMPORTANT: BAM files must be indexed (.bai) and must contain'
        ' only a single sample, identified by an @RG line with an SM '
        ' tag in the header.')
    infiles.add_argument('--bulk-bam', metavar='PATH',
        help='Matched bulk sample (not-single cell) for removing '
            'germline or clonal SNVs.  Only one sample may be '
            'designated as the bulk sample.')
    infiles.add_argument('--sc-bam', metavar='PATH', action='append',
        help='BAM corresponding to single cells that are to be '
            'analyzed.  May be specified multiple times.')
    infiles.add_argument('--bam',  action='append', metavar='PATH',
        help='Additional BAM files that will be included in GATK\'s'
            'joint genotyping, but will otherwise not affect the '
            'analysis.  May be specified several times.')
    
    
    
    gatk = config_parser.add_argument_group('Genotyping intervals',
        'These parameters allow the user to specify which portions of the genome should be genotyped.  By default, all autosomal regions will be analyzed.  Regions MUST be specified in the same order as the reference genome and should not overlap!!  The maximum target region is chrs 1-22 and X, due to the SHAPEIT reference panel.  Non-pseudoautosomal regions (PARs) on chrX may also be analyzed, but male samples may fail due to lack of hSNPs.  Initial GATK calling will be performed on each region in parallel if possible, which can greatly decrease runtime.')
    gatk.add_argument('--regions', metavar="STRING",
        help='A comma separated list of regions in GATK format: chr:start-stop, e.g. 22:30000001-31000000,22:31000001-32000000.  Cannot be specified in addition to --regions-file.')
    gatk.add_argument('--regions-file', metavar='PATH',
        help='A BED file containing regions to be analyzed.  Cannot be specified in addition to --regions.')
    
    
    
    caller = config_parser.add_argument_group("Somatic SNV calling parameters. A minimum requirement of 2 reads supporting a putative mutation is probably good practice at most sequencing depths.  However, the minimum total depth for single cell and bulk may need to be altered.  The defaults of 6 and 11, respectively, were successful on single cells with >25x and bulk with >30x mean coverage.")
    caller.add_argument('--target-fdr', type=float, metavar='FLOAT',
        help='Desired false discovery rate (FDR), a number between 0 and 1 that represents the fraction of the final call set that is expected to be false positives.  This is not formal FDR control via, e.g., q-values.  In general, lower values will increase specificity at the cost of sensitivity.')
    caller.add_argument('--snv-min-sc-alt', type=int, metavar='INT',
        help='Reject somatic SNVs with fewer than INT reads carrying the mutation in single cells.')
    caller.add_argument('--snv-min-sc-dp', type=int, metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in single cells.')
    caller.add_argument('--snv-min-bulk-dp', type=int,  metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in bulk.')
    caller.add_argument('--snv-max-bulk-alt', type=int,  metavar='INT',
        help='Reject somatic SNVs supported by more than INT reads in bulk.')
    #caller.add_argument('--snv-max-bulk-mosaic-vaf', type=float,  metavar='FLOAT',
        #help='Reject mosaic SNVs with VAF > FLOAT in bulk. E.g., --max-bulk-mosaic-vaf=0.3.')
    caller.add_argument('--indel-min-sc-alt', type=int, metavar='INT',
        help='Reject somatic indels with fewer than INT reads carrying the mutation in single cells.')
    caller.add_argument('--indel-min-sc-dp', type=int, metavar='INT',
        help='Reject somatic indels covered by fewer than INT reads in single cells.')
    caller.add_argument('--indel-min-bulk-dp', type=int,  metavar='INT',
        help='Reject somatic indels covered by fewer than INT reads in bulk.')
    caller.add_argument('--indel-max-bulk-alt', type=int,  metavar='INT',
        help='Reject somatic indels supported by more than INT reads in bulk.')
    #caller.add_argument('--indel-max-bulk-mosaic-vaf', type=float,  metavar='FLOAT',
        #help='Reject mosaic indels with VAF > FLOAT in bulk. E.g., --max-bulk-mosaic-vaf=0.3.')
    caller.add_argument('--min-base-quality-score', type=int,
        help="Passed to GATK HaplotypeCaller's --min-base-quality-score. "
             "Currently only used in GATK4 GVCF.")

    caller.add_argument('--cross-sample-panel', type=readable_file, metavar='PATH',
        help='A table of alt alleles detected over a large number of single cells and bulks. Currently used only for indel filtering but should probably also be used for SNV filtering. Indel calling is NOT precise without a panel of normals.')
    #caller.add_argument('--mosaic-snvs', action='store_true',
        #help='Enable detection of mosaic SNVs.')
    # XXX: Used to be optional, probably won't ever make optional again
    #caller.add_argument('--callable-regions', metavar='true/false/yes/no',
        ##type=lambda x: bool(distutils.util.strtobool(x)),
        #help='Determine the fraction of callable genome based on '
             #'read depth in single cells and matched bulk. This callable '
             #'genome fraction is necessary for estimating the genome-wide '
             #'mutation burden. This flag is enabled by default; to disable, '
             #'callable region calculation, use --calable-regions false.')
    
    
    phasing = config_parser.add_argument_group('Phasing options',
        'Choose SHAPEIT2 or Eagle for phasing')
    phasing.add_argument('--phaser', metavar='eagle|shapeit', type=str,
        help='SHAPEIT2 does not provide a reference panel aligned to hg38. If your data are aligned to this genome version, please use Eagle instead. Eagle is currently untested for GRCh37, but may work.')
    phasing.add_argument('--shapeit-refpanel', metavar='DIR',
        help='Path to the 1000 genomes project phase 3 SHAPEIT panel.  At this time, other phasing panels are not supported. This panel is required if phaser=shapeit.')
    phasing.add_argument('--parsimony-phasing', action='store_true',
        help='Choose the phase of each training hSNP i to minimize |VAF_(i-1) - VAF_i|.  This overrides population-based phasing and does not produce well-phased mutations.  Instead, it ensures that switching errors (caused by panel phasing) in regions with allelic imbalance do not inflate model variance or lead to "ping-pong" allele balance estimates.')
    phasing.add_argument('--eagle-refpanel', metavar='DIR',
        help='Path to the an Eagle compatible phasing panel. The reference panel is a set of BCF files, one per chromosome. File names must match the pattern ${chromosome}*.bcf. The user should take care to ensure that chromosome names contain the "chr" prefix to properly match the supplied reference genome. This parameter is required if phaser=eagle.')
    phasing.add_argument('--eagle-genmap', metavar='DIR',
        help='Path to Eagle\'s provided genetic map (can be found under the "tables" directory in the Eagle tarball). This is required if phaser=eagle.')
    
    
    abmodel = config_parser.add_argument_group("AB model fitting",
        "These parameters control the exhaustive parameter search used to fit an AB correlation function for each chromosome.  This is by far the most time consuming step of the calling process.  None of these parameters are used for subsequent AB inference.")
    abmodel.add_argument('--abmodel-n-cores', type=int, metavar='INT',
        help='Multithread with INT cores when fitting the AB model parameters.  This is the preferred way of fitting the AB model and contrasts with the chunk strategy, described below.')
    abmodel.add_argument('--abmodel-samples-per-step', type=int, metavar='INT',
        help='Sample the AB model log-likelihood function INT times during each parameter space refinement step.')
    abmodel.add_argument('--abmodel-hsnp-tile-size', type=int, metavar='INT',
        help='Approximate the AB model likelihood function for each chromosome by breaking hSNPs into non-overlapping tiles of size INT.  Larger values significantly increase runtime, but may be necessary for organisms with high SNP density (e.g., >1 hSNP per kilobase on average).')
    abmodel.add_argument('--abmodel-refine-steps', type=int, metavar='INT',
        help='Refine the parameter space for random sampling INT times.  After the first log-likelihood sampling has completed, a new parameter space is defined so as to contain the 50 points with highest log likelihood.')
    abmodel.add_argument('--abmodel-use-fit', nargs=2, action='append', metavar=('SAMPLE_ID', 'FITS.RDA'),
        help='Do not fit the AB model for sample SAMPLE_ID; instead, use the fits provided in FITS.RDA. This option may be specified multiple times.')
    abmodel.add_argument('--abmodel-chunk-strategy', action='store_true', default=False,
        help='Legacy strategy for fitting AB model parameters.  In the chunked strategy, the job of sampling --abmodel-samples-per-chunk per refinement step is split into --abmodel-chunks single threaded jobs.  This was useful before multithreading support was added to AB parameter fitting and, especially, before reduced tile sampling was found to be effective.  It is unlikely that users will ever want to opt in to this strategy.')
    abmodel.add_argument('--abmodel-chunks', type=int, metavar='INT',
        help='Parallelize AB model fitting by splitting each AB model sampling step into INT jobs per chromosome. Only used by --abmodel-chunk-strategy, which is no longer the preferred way of parallelizing AB model fitting.')
    
    
    
    spikein = config_parser.add_argument_group("hSNP spike-ins",
        'hSNP spike ins provide a way to estimate sensitivity and the effects of various filters.  hSNP spikeins are also used to train models for excess indel and read clipping, so at least a few thousand spikeins are desired.')
    spikein.add_argument('--resample-M', type=int, metavar="INT",
        help='Parameter for rejection sampling. Larger M will produce fewer hSNP spikein samples for sensitivity and total mutation burden estimation. However, M must be large enough to enable proper rejection sampling.')
    
    
    rescue = config_parser.add_argument_group("Signature-based somatic mutation rescue (scan2 rescue)")
    rescue.add_argument('--scan2-object', nargs=2, action='append', metavar=('SAMPLE_ID', 'RDA'),
        help='SCAN2 object files as generated by call_mutations. SAMPLE_ID must '
             'match the @single.cell property of the object stored in RDA. This '
             'argument can be specified multiple times to create a batch.')
    rescue.add_argument('--add-muts', metavar='CSV', type=readable_file,
        help='CSV file containing somatic mutations, one per line, to add to '
             'the batch for calculating the true mutation specrum. The file must '
             'contain at least two columns: muttype (entries in this column can '
             'have value "snv" or "indel") and mutsig (values in this column must '
             'be SBS96- or ID83-channel values; e.g. ACC:C>A for SBS96 or '
             '3:Del:R:0 for ID83)')
    rescue.add_argument('--rescue-target-fdr', metavar='FLOAT', type=float,
        help='Target FDR for mutation signature rescue. Subject to the same '
             'interpretation and caveats as --target-fdr.')


    makepanel = config_parser.add_argument_group("Cross-sample filter creation (scan2 makepanel)")
    makepanel.add_argument('--makepanel-metadata', metavar='CSV', type=readable_file,
        help='CSV file containing metadata describing the samples in the panel. '
             'Must contain columns named: sample, donor and amp. The amp column '
             'is used to determine bulk vs. single cell status; amp=bulk (case '
             'sensitive) for bulks and any other value for single cells.')


    permtool = config_parser.add_argument_group("Mutation permutation for estimating background mutation rates.")
    permtool.add_argument('--permtool-muts', metavar='CSV', type=readable_file,
        help='CSV file containing all somatic calls to be permuted. Should contain '
             'at lease the following columns: sample, chr, pos, refnt, altnt, pass, '
             'rescue. Required.')
    permtool.add_argument('--permtool-n-permutations', default=10000, type=int, metavar='N',
        help='Number of shuffled mutation sets to create.')
    permtool.add_argument('--permtool-sample', nargs=2, action='append', metavar=('SAMPLE_ID', 'SCAN2_DIR'),
        help='Each sample to be permuted must be supplied explicitly by specifying '
             'this parameter. SCAN2_DIR must point to the top level of a SCAN2 '
             'directory created by the call_mutations pipeline. At least one '
             '--permtool-sample is required for `scan2 permtool`.')
    permtool.add_argument('--permtool-bedtools-genome-file', type=readable_file, metavar='PATH',
        help='Genome file formatted for the -g option of bedtools intersect.')



    ##########################################################################
    # The 'validate' subcommand
    #
    # Tries to ensure the configuration is complete.
    validate_parser = subparsers.add_parser('validate',
        help='Validate the configuration parameters of a SCAN2 analysis.'
            '  [ATTENTION: not yet exhaustive.]')
    validate_parser.set_defaults(executor=do_validate)
    
    
    ##########################################################################
    # The 'run' subcommand
    #
    # This actually runs an analysis
    run_parser = subparsers.add_parser('run',
        help='Run a fully configured SCAN2 analysis.  Always run the '
            '"validate" subcommand first.')
    run_parser.set_defaults(executor=do_run)
    run_parser = add_runtime_args(run_parser)
    

    ##########################################################################
    # The 'show' subcommand
    #
    # Show all configuration information. Maybe allow for serialization?
    # Could just dump the YAML.
    show_parser = subparsers.add_parser('show',
        help='Show the configuration of a SCAN2 analysis.')
    show_parser.set_defaults(executor=do_show)
    show_parser.add_argument('--verbose', action='store_true', default=False,
        help='Show all configuration variables and settings.')
    
    
    ##########################################################################
    # The 'progress' subcommand
    #
    # Print some information about the progress of the run:
    #   #Jobs complete / Total #Jobs
    #   #Jobs with errors
    progress_parser = subparsers.add_parser('progress',
        help='Prints statistics regarding the progress of a SCAN2 run.')
    progress_parser.set_defaults(executor=do_progress)
    progress_parser.add_argument('-v', '--verbose', action='store_true', default=False,
        help='Show progress for individual rules.')


    ##########################################################################
    # The 'makepanel' subcommand
    #
    # Runs GATK HaplotypeCaller on a large set of BAMs, usually across cells
    # from multiple individuals. The goal is to generate a matrix of read
    # support for reference and alternate alleles at all sites with evidence
    # of non-reference bases.
    #
    # makepanel runs an analysis, so runtime_args are appropriate.
    makepanel_parser = subparsers.add_parser('makepanel',
        help='Run GATK HaplotypeCaller jointly. Produces files necessary for '
             'indel calling (particularly, the cross-sample filter). IMPORTANT: '
             'If `makepanel` output is not directly used as input to SCAN2 '
             'mutation calling, then the user should ensure that identical '
             '--gatk options are used between makepanel and run.')
    makepanel_parser.set_defaults(executor=do_makepanel)
    makepanel_parser = add_runtime_args(makepanel_parser)
    makepanel_parser.add_argument('--makepanel-n-cores', type=int, default=20, metavar='N',
        help='Number of cores to use when digesting cross-sample panel GATK output.')
    
    
    ##########################################################################
    # The 'rescue' subcommand
    #
    # Runs SCAN2 somatic mutation rescue procedures. Currently, rescue is
    # performed using mutation signatures.
    #
    # rescue runs an analysis, so runtime_args are appropriate.
    rescue_parser = subparsers.add_parser('rescue',
        help='Run SCAN2 mutation signature-based rescue to call additional '
             'somatic SNVs and indels that did not meet the requirements for '
             'VAF-based calling, but are nevertheless unlikely to be '
             'artifactual.')
    rescue_parser.set_defaults(executor=do_rescue)
    rescue_parser = add_runtime_args(rescue_parser)
    rescue_parser.add_argument('--rescue-n-cores', type=int, default=20, metavar='N',
        help='Parallelize somatic mutation rescue over N cores. Each core reads '
             'and processes a full SCAN2 object. For typical human cells, about '   
             '6 GB of RAM is necessary per core.')


    ##########################################################################
    # The 'permtool' subcommand
    #
    # Permutes mutations to create background mutation rates for enrichment
    # testing.
    #
    # permtool runs an analysis, so runtime_args are appropriate.
    permtool_parser = subparsers.add_parser('permtool',
        help='Shuffle somatic mutations around the part of the genome callable '
             'by SCAN2. Also ensures that shuffled mutations match in mutation '
             'spectrum. Importantly, the user must preprocess somatic mutations '
             'to remove, e.g., duplicated sites. SCAN2\'s digest_calls.R script '
             'performs some recommended preprocessing steps. Memory required for '
             'this pipeline is around 4 GB per core. During the '
             'make_permutations phase, bedtools shuffle can require ~2 GB of '
             'memory per thread at peak depending on the size of the callable '
             'regions file.')
    permtool_parser.set_defaults(executor=do_permtool)
    permtool_parser = add_runtime_args(permtool_parser)


    # Get the args and run the relavent subcommand
    args = ap.parse_args()
    if args.subcommand is None:
        print("ERROR: a valid subcommand must be provided. See below.")
        ap.print_help()
        exit(1)
    
    args.analysis_dir = os.path.abspath(args.analysis_dir)
    args.executor(args)
